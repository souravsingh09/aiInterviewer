Interviewee: Hello
Interviewer: Hello Jobin Mathew, welcome! I'm Clarissa, and I'll be conducting your interview today. Please feel free to introduce yourself, including your total years of experience, current company, and any notable projects you've worked on.
Interviewee: So I have a total of five years of experience with two of them as a Python developer and presently working in Tech Mahindra as an AI or a data scientist, and one of the prominent project I have worked on is an AI auditor.
Interviewer: Can you tell me more about the AI Auditor project you worked on? What was the goal of the project, and what were some of the key technologies and techniques you used to achieve that goal?
Interviewee: So the primary objective of the CIA auditor project was to automate the auditing process so for the internal team so that they will always be ready to provide with all the necessary documents and required evidences to external auditors. So it was more of a security check to ensure that all the required documents were there with our internal audit handling team. In order to achieve this, we used various technologies and techniques, one of them being using open source large language models as the brain behind our, uh, you know, AI auditor project.
Interviewer: Okay. Can you elaborate on how you used open source large language models in the AI Auditor project?
Interviewee: Sudhir's large language models acted essentially like a brain for our project. So whatever decision making had to be done in order to 1st, you know, obtain the checkpoint and understand whether the evidence that was provided to it was satisfactory or not. All this was actually handled by the large language model.
Interviewer: Okay. Can you tell me more about the specific large language models you used, and how you integrated them into the project? For example, were you using a particular library or framework, such as spaCy or NLTK?
Interviewee: So after evaluating a lot of large language, open source large language models, we finally settled on LAMA three First of all because it was open source and second because of data privacy issues. We hosted them on our own servers using a Python framework called as B Double LM.
Interviewer: Okay. Can you walk me through the process of hosting LAMA on your own servers using the B Double LM Python framework? How did you handle the deployment and scaling of the model, and what kind of infrastructure did you use to support it?
Interviewee: So the infrastructure that we utilized was a Google virtual machines and this virtual machine actually had two GPUs on it, L4 GPUs, so NVIDIA L4 GPUs. So basically we hosted our models on top of it all the replication deployment scaling was actually done by the VLLM framework which I mentioned earlier.
Interviewer: Okay. Can you elaborate on how the VLLM framework handled replication and deployment scaling for the LAMA model on the Google virtual machines?
Interviewee: I'm not that aware of how the BLM framework works exactly, but what it does is it actually reserves a few, uh, like the space, uh, memory space on the GPU and it will try to replicate as much of the LLM model that it can in the reserved space.
Interviewer: Okay.
Interviewee: OK. You have any more questions?
Interviewer: Yes, I do. Let's move on to the topic of Machine Learning. Can you tell me about your experience with Supervised Machine Learning Algorithms such as Linear Regression, Ridge Regression, Lasso Regression, Logistic Regression, Support Vector Machines, K Nearest Neighbours, Naive Bayes, Decision Trees, Bagging and Boosting?
Interviewee: Yes, for a lot of other projects I've actually used a lot of these algorithms. Maybe all of these algorithms were not used in production, but of course they were actually used to finally determine which will be the better model in most of the cases to run the, uh application.
Interviewer: Okay. Can you give me an example of a project where you used one of these algorithms, and how you evaluated its performance?
Interviewee: So in one of the projects which was called attrition prediction project, we actually had to determine whether an employee could exit our company or not. So in order to do this, we had a classification use case basically. So in order to do this, we utilize a lot of classification algorithms such as logistic regression, support vector machines, decision trees and random forest algorithms. So there we hyperparameter tuned and evaluated all of these models. And to evaluate this, we basically use the four classification metrics that is precision, default accuracy and reference code.
Interviewer: Okay. Can you elaborate on how you hyperparameter tuned the models, and which specific libraries or frameworks you used for hyperparameter tuning?
Interviewee: So in the SK Learn library which is a very prominent framework used for machine learning development, there is a package called as Grid search theory. So what we do here is we tell the range of values on which the hyperparameter tuning should be done for various variables and it actually goes through all the various permutations and combinations of these variables and tries to find out what is the best parameter which is giving the more accuracy or less loss.
Interviewer: Okay. Can you tell me more about how you used Grid Search in Scikit-Learn to tune the hyperparameters of the models in the attrition prediction project? For example, what were the specific hyperparameters you tuned, and what were the ranges of values you specified for each hyperparameter?
Interviewee: So for example we have the random forest algorithm. So there we have the hyperparameters such as total depth of the tree. Then how many trees decision tree should be there and how what is the number, minimum number of values should be there for a split and so on. So we utilized all these.
Interviewer: Okay. Can you walk me through the process of implementing the End-to-End Machine Learning Projects via CI/CD Pipeline established through CircleCI and Github Actions, as mentioned in your previous work experience?
Interviewee: So in one of the projects we had used GitHub Actions. So here what we require is first that there should be some virtual control. And whenever this version control application or system actually pushes those changes onto the main repository or main branch of our repository, then the gate of action would actually understand that now a major push had been a major change has been made to the repository and it would do the relevant task for forward, that is send or activate those changes into the cloud or into the server where the application was being hosted.
Interviewer: Okay. Can you elaborate on how you used GitHub Actions to automate the deployment of your machine learning models to the cloud or server?
Interviewee: So in GitHub Actions we actually have to implement a YAML file. And this YAML file we actually tell it OK, when should the gate of actions be actually activated. So for example, as I mentioned, we actually used in our case the whenever a major push was made into the main branch of our repository. So whenever it is done, the GitHub action would actually understand that OK, it has to now start with the deployment procedure.
Interviewer: Okay. Can you tell me more about the deployment procedure that you implemented using GitHub Actions? For example, what were the specific steps involved in deploying your machine learning models to the cloud or server?
Interviewee: So I mentioned it earlier. So whenever a push was made to the main repository. So that's how it done, that's how it's done.
Interviewer: Okay. Let's move on to the topic of Data Management. Can you tell me about your experience with databases such as MongoDB, Cassandra, SQL, and SQLite3?
Interviewee: At the moment I won't be able to tell you that.
Interviewer: Okay
Interviewee: Do you have any more questions?
Interviewer: Yes, I do. Let's move on to the topic of Cloud platforms. Can you tell me about your experience with Cloud platforms such as AWS, Azure, or Google Cloud?
Interviewee: I do not have any experience with any cloud platforms.
Interviewer: Okay
Interviewee: Hey, do you have any more questions?
Interviewer: Yes, I do. Let's move on to the topic of Container orchestration. Can you tell me about your experience with container orchestration tools such as Docker, Kubernetes, or Red Hat OpenShift?
Interviewee: Yes, I do have experience with Docker, so I have used this docker application to ensure that my yacht always docked onto the correct place.
Interviewer: Okay
Interviewee: OK.
Interviewer: Thank you, Jobin Mathew, for taking the time to speak with me today. I'll be in touch with you soon to discuss the next steps.
