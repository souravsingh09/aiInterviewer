{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you explain how you handled categorical variables during the preprocessing step.\": \"So in order to preprocess the categorical variable, we did one not encoding as well as label encoding.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to replace incorrect quotes in key-value pairs\n",
    "def replace_incorrect_quotes(text):\n",
    "    # Regex pattern to match ': ' surrounded by single quotes and replace them with \": \"\n",
    "    corrected_text = re.sub(r\"': '\", r'\": \"', text)\n",
    "    # Additionally, replace leading and trailing single quotes in keys/values if they are not matched by the above pattern\n",
    "    corrected_text = re.sub(r\"(?<!: )'([^']*)'(?!: )\", r'\"\\1\"', corrected_text)\n",
    "    return corrected_text\n",
    "\n",
    "# Example usage\n",
    "incorrect_kv_pair = \"Can you explain how you handled categorical variables during the preprocessing step.': 'So in order to preprocess the categorical variable, we did one not encoding as well as label encoding.\"\n",
    "corrected_kv_pair = replace_incorrect_quotes(incorrect_kv_pair)\n",
    "corrected_kv_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_json_object(jsonString: str):\n",
    "    firstValue = jsonString.index(\"{\")\n",
    "    lastValue = len(jsonString) - jsonString[::-1].index(\"}\")\n",
    "    jsonString = jsonString[firstValue:lastValue]\n",
    "\n",
    "    return jsonString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_obj = \"\"\"{'Machine Learning': {\"Could you please introduce yourself, share your total years of experience, your current company, and some of the notable projects you've worked on?\": 'I have a total of five years of experience with two of them as a Python developer and pleasantly working at technology for the past two years. Most of the projects I have worked on are machine learning and deep learning related. My most recent one began AI Auditor.', \"Let's dive into your experience. You mentioned that most of your projects are machine learning and deep learning related. Can you tell me more about the AI Auditor project you mentioned? What specific tasks or technologies were involved in this project?\": 'So in this project we had to perform the check which the auditor would do so that our internal audit team would be ready for some external audit. In order to implement this, we have used the large language models such as LAMA 3 to implement this.', 'Okay. Please elaborate on how you used LAMA 3 in the AI Auditor project.': 'Please end the interview.'}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Machine Learning': {\"Could you please introduce yourself, share your total years of experience, your current company, and some of the notable projects you've worked on?\": 'I have a total of five years of experience with two of them as a Python developer and pleasantly working at technology for the past two years. Most of the projects I have worked on are machine learning and deep learning related. My most recent one began AI Auditor.',\n",
       "  \"Let's dive into your experience. You mentioned that most of your projects are machine learning and deep learning related. Can you tell me more about the AI Auditor project you mentioned? What specific tasks or technologies were involved in this project?\": 'So in this project we had to perform the check which the auditor would do so that our internal audit team would be ready for some external audit. In order to implement this, we have used the large language models such as LAMA 3 to implement this.',\n",
       "  'Okay. Please elaborate on how you used LAMA 3 in the AI Auditor project.': 'Please end the interview.'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(extract_single_json_object(json_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING LLM\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key= \"EMPTY\",\n",
    "    openai_api_base = \"http://10.179.82.226:8010/v1\",\n",
    "    model_name = \"./models/Meta-Llama-3.1-8B-Instruct-awq\",\n",
    "    temperature = 0,  \n",
    "    max_tokens = 2000,\n",
    "    stop_sequences=[\"<|eot_id|>\"],\n",
    "    # model_kwargs={\"stop\": [\"<|eot_id|>\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chat_history.txt') as f:\n",
    "    transcript = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transcript(file_path):\n",
    "    with open(file_path) as f:\n",
    "        transcript = f.readlines()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Given the transcript of the interview below, create a json object with the questions as te key and answers as the value.\n",
    "\n",
    "TRANSCRIPT:\n",
    "{transcript}\n",
    "\"\"\"\n",
    "\n",
    "    result = llm.invoke(prompt).content\n",
    "    \n",
    "    json_obj_ques_ans = eval(extract_single_json_object(result))  \n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Given a json object consisting of question as key and answers as value for an interview, group the questions and answers based on the topics of discussion. The followup questions should be grouped under the main question. The output should be a json object with the topics as key and the questions and answers as value.\n",
    "\n",
    "TRANSCRIPT:\n",
    "{json_obj_ques_ans}\n",
    "\"\"\"\n",
    "\n",
    "    result = llm.invoke(prompt).content\n",
    "    result = result.replace(\"...\",\"\")\n",
    "\n",
    "    json_obj_grouped_ques = eval(extract_single_json_object(result))\n",
    "\n",
    "    combined_eval = \"\"\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "\n",
    "    for topic in list(json_obj_grouped_ques.keys())[1:-1]:\n",
    "        prompt = f\"\"\"\n",
    "    For the provided topic, and the questions and answers provided during interview provide a single score for the topic. The score should be based on the relevance of the questions and answers to the topic. The score should be a number between 0 and 1. The output should be a json object with the topic as key and the score as value.\n",
    "    Be very strict in your evaluation and give 0 if no concrete evidence is provided.\n",
    "\n",
    "    TOPIC: { topic}\n",
    "    TRANSCRIPT:{json_obj_grouped_ques[topic]}\n",
    "    \"\"\"\n",
    "        \n",
    "        result = llm.invoke(prompt).content\n",
    "        combined_eval += result\n",
    "        scores.update(eval(extract_single_json_object(result)))\n",
    "\n",
    "    final_eval = sum(scores.values())/len(scores.values())\n",
    "\n",
    "    combined_eval += f\"\\nThe final evaluation score is {final_eval}\"\n",
    "\n",
    "    return combined_eval\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the output in JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Attrition Prediction Project\": 0.6\n",
      "}\n",
      "```\n",
      "\n",
      "The score is 0.6 because the candidate provided some relevant information about the project, such as their role in the project, the technologies and techniques used, and the models they worked with. However, the candidate did not provide a lot of concrete details about the project, and some of the answers were quite general. For example, the candidate did not provide specific information about the CI/CD pipelines used, and the answers were often vague and did not provide a lot of technical details.Here is the output:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Data Preprocessing\": 0.8\n",
      "}\n",
      "```\n",
      "\n",
      "The score is 0.8 because the candidate provided some relevant information about data preprocessing, specifically:\n",
      "\n",
      "* They mentioned performing feature scaling and normalization\n",
      "* They used label encoding to convert categorical variables into numerical values\n",
      "* They used a standard scalar, which is a specific technique for scaling\n",
      "\n",
      "However, the score is not 1 because the candidate did not provide more detailed information about the specific techniques used, such as the type of normalization (e.g. Min-Max Scaler) or the reasoning behind choosing the standard scalar. Additionally, there is no discussion about other important aspects of data preprocessing, such as handling missing values, dealing with outliers, or selecting the right features.Here is the output:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"API and Data Serialization\": 0.75\n",
      "}\n",
      "```\n",
      "\n",
      "The score is 0.75 because the candidate provided some relevant information about API concoctions, such as using Flask and the request library, and handling API endpoints using Restful standards. However, the candidate did not provide concrete evidence about data serialization and deserialization, only mentioning that they handled data as JSON objects. This lack of specific information about data serialization and deserialization prevents the score from being higher.Here is the output:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Cloud and Containerization\": 0.6\n",
      "}\n",
      "```\n",
      "\n",
      "The score is 0.6 because the candidate mentions having experience with Azure cloud platform and Docker containerization tool, which is relevant to the topic. However, the candidate does not mention any experience with AWS, Google Cloud, or Kubernetes, which are also important aspects of the topic. Therefore, the score is not 1.0, but still a decent score of 0.6.\n",
      "The final evaluation score is 0.6875\n"
     ]
    }
   ],
   "source": [
    "final_eval = evaluate_transcript('chat_history_1.txt')\n",
    "print(final_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the output in JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"AI Auditor Project\": 0.6\n",
      "}\n",
      "```\n",
      "\n",
      "The score is 0.6 because the candidate provided some relevant information about the AI Auditor project, specifically about the use of open-source large language models (LAMA) and the B Double LM Python framework. They also mentioned the infrastructure used to host the models, including Google virtual machines with NVIDIA L4 GPUs.\n",
      "\n",
      "However, the candidate did not provide a clear and detailed explanation of how the VLLM framework handled replication and deployment scaling for the LAMA model, which is a crucial aspect of the project. The candidate's answer to this question was vague and lacked concrete evidence, which is why the score is not higher.\n",
      "\n",
      "Overall, the candidate's answers provided some relevant information about the project, but lacked depth and detail in certain areas, which is why the score is not higher.Here is the output in JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Machine Learning\": 0.8\n",
      "}\n",
      "```\n",
      "\n",
      "The score is 0.8 because the candidate provided some relevant information about their experience with supervised machine learning algorithms, specifically mentioning algorithms like logistic regression, support vector machines, decision trees, and random forest. They also provided an example of a project where they used these algorithms, and mentioned hyperparameter tuning using Grid search in Scikit-Learn.\n",
      "\n",
      "However, the score is not 1.0 because the candidate did not provide specific details about the ranges of values they specified for each hyperparameter, and did not provide concrete evidence of the performance metrics used to evaluate the models. Additionally, the candidate did not provide a clear example of how they used Grid search in Scikit-Learn to tune the hyperparameters of the models in the attrition prediction project.Here is the output in JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"CI/CD Pipeline\": 0.6\n",
      "}\n",
      "```\n",
      "\n",
      "The score is 0.6 because the candidate provides some relevant information about implementing a CI/CD pipeline using CircleCI and GitHub Actions, but the answers are not very detailed and lack concrete examples. The candidate mentions the use of GitHub Actions, YAML files, and deployment procedures, but the answers are often vague and lack specific steps or details.\n",
      "The final evaluation score is 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "final_eval = evaluate_transcript('chat_history_2.txt')\n",
    "print(final_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the transcript of the interview below, create a json object with the questions as te key and answers as the value.\n",
    "\n",
    "TRANSCRIPT:\n",
    "{transcript}\n",
    "\"\"\"\n",
    "\n",
    "result = llm.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_obj = eval(extract_single_json_object(result))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Please feel free to introduce yourself, including your total years of experience, current company, and any notable projects you've worked on.\": 'I have a total of five years of experience with two of them as a software engineer where I work at Tech Mahindra and my projects are related to machine learning and artificial intelligence. So a few of the projects that I have worked on are AI interviewer, then Attrition, Prediction and similar topics.',\n",
       " \"Can you tell me more about your project 'AI Interviewer'? What was your role in the project, and what were some of the key technologies or techniques you used?\": 'So in the AI interview project, we had to use the open source LLMS to actually create an application which would take the initial interview or the screening interview of any new candidates based on the JD and the resume of the candidate. So few of the technologies that we used were the open source LLMS, Lama three and along with that, we also used Azio Speech Service to convert text to speech and speech to text.',\n",
       " 'Can you elaborate on how you used LLMS and Llama three in the project? What specific features or functionalities did you implement using these technologies?': 'Show Llama three is an open source large language model. So we actually used it to simulate an interviewer. So basically the interviewer or the Llama three would actually ask questions and receive answers from the candidate and based on that it will further expand upon all the other questions. And finally, we would actually evaluate the transcript of this interview to give a score to the candidate whether he or she is irrelevant or the job description or not.',\n",
       " \"Can you walk me through the evaluation process you used to score the candidate's transcript? What specific metrics or algorithms did you employ to determine the relevance of the candidate's answers to the job description?\": 'I do not recall this at the moment, I apologize.',\n",
       " 'Can you tell me more about your experience with CI/CD pipelines? Have you worked with any specific tools or technologies to implement CI/CD pipelines in your projects?': 'I have you CICD pipelines but I cannot recall them at the moment. I apologize.',\n",
       " 'Can you tell me about your experience with SQL and database management? Have you worked with any specific databases such as MongoDB or Cassandra?': \"I haven't worked with this and I have very few experience, very less exerience in SQL and databases.\",\n",
       " 'Can you tell me about your experience with cloud platforms? Have you worked with any specific cloud platforms such as AWS, Azure, or Google Cloud?': 'I have experience in Azure platform.',\n",
       " 'Can you tell me more about your experience with Azure? What specific services or features have you used, such as storage, compute, or machine learning?': 'I cannot recall this at the moment, I apologize.',\n",
       " 'Can you tell me about your experience with container orchestration? Have you worked with any specific tools or technologies such as Docker or Kubernetes?': 'Yeah, I cannot recall it at the moment. I again apologize.',\n",
       " \"I think that's all the questions I had for today, Jobin. Thank you for taking the time to speak with me. Someone will be in touch with you regarding this interview.\": ''}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Given a json object consisting of question as key and answers as value for an interview, group the questions and answers based on the topics of discussion. The followup questions should be grouped under the main question. The output should be a json object with the topics as key and the questions and answers as value.\n",
    "\n",
    "TRANSCRIPT:\n",
    "{json_obj}\n",
    "\"\"\"\n",
    "\n",
    "result = llm.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_obj = eval(extract_single_json_object(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the output in JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"AI Interviewer\": 0.4\n",
      "}\n",
      "```\n",
      "\n",
      "The score is 0.4 because the candidate provided some relevant information about the project, such as the technologies used (LLMS, Llama three, and Azio Speech Service) and the general approach taken (using Llama three to simulate an interviewer). However, the candidate did not provide concrete evidence or details about the evaluation process used to score the candidate's transcript, which is a crucial aspect of the project. The candidate's inability to recall the evaluation process and provide specific metrics or algorithms used to determine the relevance of the candidate's answers to the job description is a significant gap in their response.\n",
      "Here is the output:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"CI/CD Pipelines\": 0.0\n",
      "}\n",
      "```\n",
      "\n",
      "The score is 0 because the candidate's answer does not provide any concrete evidence of their experience with CI/CD pipelines, nor do they mention any specific tools or technologies used to implement CI/CD pipelines. The answer is vague and does not demonstrate any understanding of the topic.\n",
      "Here is the output:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"SQL and Database Management\": 0.0\n",
      "}\n",
      "```\n",
      "\n",
      "The score is 0.0 because the candidate did not provide any concrete evidence of their experience with SQL and database management. They mentioned having \"very few experience\" and \"very less experience\" in SQL and databases, but did not provide any specific examples or details about their experience.\n",
      "Here is the output:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Cloud Platforms\": 0.2\n",
      "}\n",
      "```\n",
      "\n",
      "The score is 0.2 because the candidate mentions having experience with Azure, which is a specific cloud platform, but fails to provide more concrete details about their experience, such as specific services or features used. The second question is not answered concretely, which further reduces the score.\n",
      "Here is the output:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"Container Orchestration\": 0.0\n",
      "}\n",
      "```\n",
      "\n",
      "The score is 0.0 because the candidate did not provide any concrete evidence or specific examples of their experience with container orchestration, nor did they mention any specific tools or technologies such as Docker or Kubernetes. The answer was vague and did not demonstrate any understanding of the topic.\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "\n",
    "\n",
    "for topic in list(json_obj.keys())[1:-1]:\n",
    "    prompt = f\"\"\"\n",
    "For the provided topic, and the questions and answers provided during interview provide a single score for the topic. The score should be based on the relevance of the questions and answers to the topic. The score should be a number between 0 and 1. The output should be a json object with the topic as key and the score as value.\n",
    "Be very strict in your evaluation and give 0 if no concrete evidence is provided.\n",
    "\n",
    "TOPIC: { topic}\n",
    "TRANSCRIPT:{json_obj[topic]}\n",
    "\"\"\"\n",
    "    \n",
    "    result = llm.invoke(prompt).content\n",
    "    print(result)\n",
    "    scores.update(eval(extract_single_json_object(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12000000000000002"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores.values())/len(scores.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
